{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of lab1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fllay/lab/blob/main/tensorflowjstest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x2WYDZURyoe"
      },
      "source": [
        "!nvidia-smi "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdvZUSq-XSdS"
      },
      "source": [
        "!pip install flask\n",
        "!pip install flask_cors\n",
        "!apt-get install -qq protobuf-compiler python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib PyDrive\n",
        "!pip install -q pycocotools\n",
        "!pip install tf_slim\n",
        "!pip install tf-models-official\n",
        "!pip install flask-executor\n",
        "\n",
        "%mkdir /content/uploads\n",
        "%mkdir /content/images\n",
        "%mkdir /content/images/test\n",
        "%mkdir /content/images/train\n",
        "%mkdir /content/training\n",
        "%mkdir /content/inference_graph\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMrmKq3qH6EX"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfMTDfhai9kk"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone the tensorflow models repository if it doesn't already exist\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX8LWFPmi-57"
      },
      "source": [
        "%%bash\n",
        "cd /content/models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3-EJz0F5a2V"
      },
      "source": [
        "import os\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "from google.protobuf import text_format\n",
        "from object_detection.protos.string_int_label_map_pb2 import StringIntLabelMap, StringIntLabelMapItem\n",
        "from object_detection.protos import pipeline_pb2\n",
        "\n",
        "%cd /content/models/research\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZC6seev72Dx"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import tensorflow as tf\n",
        "def xml_to_csv(path):\n",
        "    xml_list = []\n",
        "    for xml_file in glob.glob(path + '/*.xml'):\n",
        "        #print(xml_file)\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall('object'):\n",
        "            \n",
        "            try:\n",
        "              #print(member[0].text)\n",
        "              value = (root.find('filename').text,\n",
        "                     int(root.find('size')[0].text),\n",
        "                     int(root.find('size')[1].text),\n",
        "                     member[0].text,\n",
        "                     int(member[4][0].text),\n",
        "                     int(member[4][1].text),\n",
        "                     int(member[4][2].text),\n",
        "                     int(member[4][3].text)\n",
        "                     )\n",
        "              xml_list.append(value)\n",
        "            except IndexError:\n",
        "              pass\n",
        "    \n",
        "            \n",
        "    column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "    xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
        "    return xml_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-7fBmT6CnXz"
      },
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "from collections import namedtuple, OrderedDict\n",
        "import io\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "\n",
        "\n",
        "def create_tf_example(group, path, label_dict):\n",
        "    with tf1.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(label_dict[row['class']])\n",
        "\n",
        "    tf_example = tf1.train.Example(features=tf1.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "def split_f(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def gen_tfrecord(image_dir, csv_input, output_path, label_dict):\n",
        "    writer = tf1.python_io.TFRecordWriter(output_path)\n",
        "    path = os.path.join(image_dir)\n",
        "    examples = pd.read_csv(csv_input)\n",
        "    grouped = split_f(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path, label_dict)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
        "\n",
        "def convert_classes(classes, start=1):\n",
        "    msg = StringIntLabelMap()\n",
        "    label_dict = {}\n",
        "    for id, name in enumerate(classes, start=start):\n",
        "      msg.item.append(StringIntLabelMapItem(id=id, name=name))\n",
        "      label_dict[name] = id\n",
        "\n",
        "    text = str(text_format.MessageToBytes(msg, as_utf8=True), 'utf-8')\n",
        "    return text, label_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOeLrtmBYnkQ"
      },
      "source": [
        "%cd /content\n",
        "!wget http://download.tensorflow.org/models/object_detection/classification/tf2/20200710/mobilenet_v2.tar.gz\n",
        "!tar -xvf mobilenet_v2.tar.gz\n",
        "!rm mobilenet_v2.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAs0KRMvL8vu"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/ssd_mobilenet_v2_320x320_coco17_tpu-8.config\n",
        "!cp ssd_mobilenet_v2_320x320_coco17_tpu-8.config mobilenet_v2.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvkzkrhJj_Ua"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/hugozanini/object-detection/master/inferenceutils.py\n",
        "from inferenceutils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANZC7w27rtTS"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/fllay/lab.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bWvwuCQbXPh"
      },
      "source": [
        "NUM_STEPS = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK4Wjic7CPtn"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2CIOfcRXwHf"
      },
      "source": [
        "from flask import Flask, render_template, request, abort, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "from os import path\n",
        "from flask import Flask, flash, make_response, request, redirect, render_template\n",
        "from werkzeug.utils import secure_filename\n",
        "from flask_executor import Executor\n",
        "import shutil\n",
        "import zipfile\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "from multiprocessing import Queue\n",
        "\n",
        "PATH = '/content'\n",
        "\n",
        "TEMPLATE_DIR = os.path.abspath(PATH +'/lab/lab1')\n",
        "UPLOAD_FOLDER = os.path.join(PATH, 'uploads')\n",
        "IMAGE_FOLDER = os.path.join(PATH, 'images')\n",
        "IMAGE_FOLDER_TEST = os.path.join(PATH, 'images', 'test')\n",
        "IMAGE_FOLDER_TRAIN = os.path.join(PATH, 'images','train')\n",
        "\n",
        "TEST_SIZE = 10\n",
        "\n",
        "output_q = Queue()\n",
        "\n",
        "def run(cmd):\n",
        "\n",
        "  output = get_ipython().getoutput(cmd)\n",
        "  print(\"###################\")\n",
        "  print(output)\n",
        "  output_q.put(output)\n",
        "\n",
        "\n",
        "app = Flask(__name__, template_folder=TEMPLATE_DIR)\n",
        "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
        "executor = Executor(app)\n",
        "#CORS(app)\n",
        "\n",
        "\n",
        "def train():\n",
        "  model_dir = '/content/training/'\n",
        "  labelmap_path = '/content/images/object-detection.pbtxt'\n",
        "  pipeline_config_path = '/content/mobilenet_v2.config'\n",
        "  fine_tune_checkpoint = '/content/mobilenet_v2/mobilenet_v2.ckpt-1'\n",
        "  num_steps = NUM_STEPS\n",
        "  num_eval_steps = 100\n",
        "  run('python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path=/content/mobilenet_v2.config \\\n",
        "    --model_dir=/content/training/ \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps=2000 \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --num_eval_steps=100')\n",
        "  return 0\n",
        "\n",
        "\n",
        "def export_model():\n",
        "  model_dir = '/content/training/'\n",
        "  labelmap_path = '/content/images/object-detection.pbtxt'\n",
        "  pipeline_config_path = '/content/mobilenet_v2.config'\n",
        "  fine_tune_checkpoint = '/content/mobilenet_v2/mobilenet_v2.ckpt-1'\n",
        "  num_steps = NUM_STEPS\n",
        "  num_eval_steps = 100\n",
        "  output_directory = '/content/inference_graph'\n",
        "  run('python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir /content/training/ \\\n",
        "    --output_directory /content/inference_graph \\\n",
        "    --pipeline_config_path /content/mobilenet_v2.config')\n",
        "  return 0\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "    #return 'Web App with Python Flask!'\n",
        "\n",
        "@app.route('/hello', methods=['GET'])\n",
        "def hello():\n",
        "    print(\"hello called\")\n",
        "    message = {\n",
        "          'message': \"Hello world222\"\n",
        "    }\n",
        "    return jsonify(message)\n",
        "\n",
        "@app.route('/trainObject', methods=['POST','PUT'])\n",
        "def trainObject():\n",
        "  executor.submit_stored('runcmd', train) \n",
        "  return make_response((\"Finish training\", 200))\n",
        " \n",
        "\n",
        "@app.route('/exportModelObject', methods=['POST','PUT'])\n",
        "def exportModelObject():\n",
        "  executor.submit_stored('runcmd', export_model)\n",
        "  return make_response((\"Finish export\", 200))\n",
        "\n",
        "@app.route('/getResult',  methods=['POST','PUT'])\n",
        "def get_result():\n",
        "    if not executor.futures.done('runcmd'):\n",
        "      if not output_q.empty():\n",
        "        print('!!!!!!!!!!')\n",
        "        print(output_q.get())\n",
        "      return jsonify({'status': executor.futures._state('runcmd')})\n",
        "    return jsonify({'status': \"done\"})\n",
        "\n",
        "@app.route('/split', methods=['POST','PUT'])\n",
        "def split_route():\n",
        "    S_PATH = os.path.join(IMAGE_FOLDER, 'ttimage')\n",
        "    print(S_PATH)\n",
        "    files_xml=glob.glob(S_PATH+'/*.xml')\n",
        "    files_png=[]\n",
        "    file_xml_test=[]\n",
        "    file_png_test=[]\n",
        "    for i in range(TEST_SIZE):\n",
        "      #files_png.append(os.path.splitext(f)[0]+\".png\")\n",
        "      file_xml_test.append(files_xml.pop(random.randrange(len(files_xml))))\n",
        "    print(files_xml)\n",
        "    print(file_xml_test)\n",
        "    for f in file_xml_test:\n",
        "      png_file = os.path.splitext(f)[0]+\".png\"\n",
        "      jpg_file = os.path.splitext(f)[0]+\".jpg\"\n",
        "      if path.exists(png_file):\n",
        "        file_png_test.append(png_file)\n",
        "      elif path.exists(jpg_file):\n",
        "        file_png_test.append(jpg_file)\n",
        "    for f in files_xml:\n",
        "      png_file = os.path.splitext(f)[0]+\".png\"\n",
        "      jpg_file = os.path.splitext(f)[0]+\".jpg\"\n",
        "      if path.exists(png_file):\n",
        "        files_png.append(png_file)\n",
        "      elif path.exists(jpg_file):\n",
        "        files_png.append(jpg_file)\n",
        "    print(files_png)\n",
        "    print(file_png_test)\n",
        "    for f in files_xml:\n",
        "      fname = os.path.basename(f)\n",
        "      dest = IMAGE_FOLDER_TRAIN +'/' + fname\n",
        "      shutil.copyfile(f, dest)\n",
        "    for f in files_png:\n",
        "      fname = os.path.basename(f)\n",
        "      dest = IMAGE_FOLDER_TRAIN +'/' + fname\n",
        "      shutil.copyfile(f, dest)\n",
        "    for f in file_xml_test:\n",
        "      fname = os.path.basename(f)\n",
        "      dest = IMAGE_FOLDER_TEST +'/' + fname\n",
        "      shutil.copyfile(f, dest)\n",
        "    for f in file_png_test:\n",
        "      fname = os.path.basename(f)\n",
        "      dest = IMAGE_FOLDER_TEST +'/' + fname\n",
        "      shutil.copyfile(f, dest)\n",
        "\n",
        "    xml_df=xml_to_csv(IMAGE_FOLDER_TRAIN)\n",
        "    print(IMAGE_FOLDER + '/train_labels.csv')\n",
        "    xml_df.to_csv(IMAGE_FOLDER + '/train_labels.csv', index=None)\n",
        "    xml_df=xml_to_csv(IMAGE_FOLDER_TEST)\n",
        "    print(IMAGE_FOLDER + '/train_labels.csv')\n",
        "    xml_df.to_csv(IMAGE_FOLDER + '/test_labels.csv', index=None)  \n",
        "\n",
        "    csv_file_train = pd.read_csv(IMAGE_FOLDER + '/train_labels.csv') \n",
        "    column_val_list_train = csv_file_train['class']\n",
        "    all_labels_train = set(column_val_list_train) \n",
        "\n",
        "    csv_file_test = pd.read_csv(IMAGE_FOLDER + '/test_labels.csv') \n",
        "    column_val_list_test = csv_file_test['class']\n",
        "    all_labels_test = set(column_val_list_test) \n",
        "\n",
        "    all_labels = all_labels_train.union(all_labels_test)\n",
        "    print(\"All labels = \"+ str(all_labels))\n",
        "\n",
        "    s_labels = list(all_labels)\n",
        "    print(\"All classes = \"+ str(s_labels))\n",
        "\n",
        "    NUM_OF_CLASS = len(s_labels)\n",
        "    print(NUM_OF_CLASS)\n",
        "    txt, label_dict = convert_classes(s_labels)\n",
        "    print(txt)\n",
        "\n",
        "    with open(IMAGE_FOLDER + '/object-detection.pbtxt', 'w') as f:\n",
        "      f.write(txt)\n",
        "\n",
        "    gen_tfrecord(image_dir=IMAGE_FOLDER_TRAIN, csv_input=IMAGE_FOLDER + '/train_labels.csv', output_path=IMAGE_FOLDER +'/train.record', label_dict=label_dict)\n",
        "    gen_tfrecord(image_dir=IMAGE_FOLDER_TEST, csv_input=IMAGE_FOLDER + '/test_labels.csv', output_path=IMAGE_FOLDER + '/test.record', label_dict=label_dict)\n",
        "\n",
        "    \n",
        "    num_classes = NUM_OF_CLASS\n",
        "    batch_size = 96\n",
        "    num_steps = NUM_STEPS\n",
        "    num_eval_steps = 100\n",
        "\n",
        "    train_record_path = IMAGE_FOLDER +'/train.record'\n",
        "    test_record_path = IMAGE_FOLDER + '/test.record'\n",
        "    model_dir = '/content/training/'\n",
        "    labelmap_path = '/content/images/object-detection.pbtxt'\n",
        "\n",
        "    pipeline_config_path = '/content/mobilenet_v2.config'\n",
        "    fine_tune_checkpoint = '/content/mobilenet_v2/mobilenet_v2.ckpt-1'\n",
        "\n",
        "    filenameOut = pipeline_config_path\n",
        "    filename = '/content/ssd_mobilenet_v2_320x320_coco17_tpu-8.config'\n",
        "    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig() \n",
        "\n",
        "    \n",
        "    '''with tf.gfile.GFile(filename, \"r\") as f:                                                                                                                                                                                                                     \n",
        "      proto_str = f.read()                                                                                                                                                                                                                                          \n",
        "      text_format.Merge(proto_str, pipeline_config)  \n",
        "\n",
        "    pipeline_config.model.ssd.num_classes = NUM_OF_CLASS\n",
        "    pipeline_config.train_config.num_steps = num_steps\n",
        "    pipeline_config.train_config.fine_tune_checkpoint=fine_tune_checkpoint\n",
        "    pipeline_config.train_input_reader.label_map_path=labelmap_path\n",
        "    pipeline_config.train_input_reader.tf_record_input_reader.input_path[0]=train_record_path\n",
        "    pipeline_config.eval_input_reader[0].label_map_path=labelmap_path\n",
        "    pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[0]=test_record_path \n",
        "    pipeline_config.train_config.batch_size=batch_size\n",
        "\n",
        "    config_text = text_format.MessageToString(pipeline_config)   \n",
        "                                                                                                                                                                                                    \n",
        "    with tf.gfile.Open(filenameOut, \"wb\") as f:                                                                                                                                                                                                                       \n",
        "      f.write(config_text)   '''\n",
        "\n",
        "    with open(pipeline_config_path) as f:\n",
        "      config = f.read()\n",
        "\n",
        "    with open(pipeline_config_path, 'w') as f:\n",
        "      # Set labelmap path\n",
        "      config = re.sub('label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(labelmap_path), config)\n",
        "      # Set fine_tune_checkpoint path\n",
        "      config = re.sub('fine_tune_checkpoint: \".*?\"', 'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), config)\n",
        "      # Set train tf-record file path\n",
        "      config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_path), config)\n",
        "      # Set test tf-record file path\n",
        "      config = re.sub('(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(test_record_path), config)\n",
        "      # Set number of classes.\n",
        "      config = re.sub('num_classes: [0-9]+', 'num_classes: {}'.format(num_classes), config)\n",
        "      # Set batch size\n",
        "      config = re.sub('batch_size: [0-9]+', 'batch_size: {}'.format(batch_size), config)\n",
        "      # Set training steps\n",
        "      config = re.sub('num_steps: [0-9]+', 'num_steps: {}'.format(num_steps), config)\n",
        "      f.write(config)\n",
        "\n",
        "    return make_response((\"Got it\", 200))\n",
        "\n",
        "@app.route('/upload', methods=['POST','PUT'])\n",
        "def upload():\n",
        "    file = request.files['file']\n",
        "\n",
        "    save_path = os.path.join(app.config['UPLOAD_FOLDER'], secure_filename(file.filename))\n",
        "    current_chunk = int(request.form['dzchunkindex'])\n",
        "    print(save_path)\n",
        "\n",
        "    # If the file already exists it's ok if we are appending to it,\n",
        "    # but not if it's new file that would overwrite the existing one\n",
        "    if os.path.exists(save_path) and current_chunk == 0:\n",
        "        # 400 and 500s will tell dropzone that an error occurred and show an error\n",
        "        \n",
        "        print(current_chunk)\n",
        "        return make_response(('File already exists', 400))\n",
        "\n",
        "    try:\n",
        "        with open(save_path, 'ab') as f:\n",
        "            f.seek(int(request.form['dzchunkbyteoffset']))\n",
        "            f.write(file.stream.read())\n",
        "    except OSError:\n",
        "        # log.exception will include the traceback so we can see what's wrong \n",
        "        print('Could not write to file')\n",
        "        return make_response((\"Not sure why,\"\n",
        "                              \" but we couldn't write the file to disk\", 500))\n",
        "\n",
        "    total_chunks = int(request.form['dztotalchunkcount'])\n",
        "\n",
        "    if current_chunk + 1 == total_chunks:\n",
        "        # This was the last chunk, the file should be complete and the size we expect\n",
        "        if os.path.getsize(save_path) != int(request.form['dztotalfilesize']):\n",
        "            print(f\"File {file.filename} was completed, \"\n",
        "                      f\"but has a size mismatch.\"\n",
        "                      f\"Was {os.path.getsize(save_path)} but we\"\n",
        "                      f\" expected {request.form['dztotalfilesize']} \")\n",
        "            return make_response(('Size mismatch', 500))\n",
        "        else:\n",
        "            print(f'File {file.filename} has been uploaded successfully')\n",
        "            with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
        "              zip_ref.extractall('/content/images')\n",
        "            \n",
        "    else:\n",
        "        print(f'Chunk {current_chunk + 1} of {total_chunks} '\n",
        "                  f'for file {file.filename} complete')\n",
        "        \n",
        "        \n",
        "        \n",
        "    res1= jsonify({\"status\":\"success\",\"info\":\"com1\",\"file_link\":1})\n",
        "    print(res1)\n",
        "    return make_response((res1, 200))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))\n",
        "app.run(host='0.0.0.0', port=5000)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxO1X9NuGC2F"
      },
      "source": [
        "#Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6RzSGipZjfa"
      },
      "source": [
        "!pip install flask-executor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8p5gBf0eyi9"
      },
      "source": [
        "  run('python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path=/content/mobilenet_v2.config \\\n",
        "    --model_dir=/content/training/ \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps=2000 \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --num_eval_steps=100')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPsdEb-pwnBP"
      },
      "source": [
        "model_dir = '/content/training/'\n",
        "labelmap_path = '/content/images/object-detection.pbtxt'\n",
        "pipeline_config_path = '/content/mobilenet_v2.config'\n",
        "fine_tune_checkpoint = '/content/mobilenet_v2/mobilenet_v2.ckpt-1'\n",
        "num_steps = NUM_STEPS\n",
        "num_eval_steps = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R78cvpkDPBvd"
      },
      "source": [
        "\n",
        "\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1 \\\n",
        "    --num_eval_steps={num_eval_steps}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-K5LpZKckmf"
      },
      "source": [
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_config_path} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --checkpoint_dir={model_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s0rKm9ef8So"
      },
      "source": [
        "output_directory = '/content/inference_graph'\n",
        "\n",
        "!python /content/models/research/object_detection/exporter_main_v2.py \\\n",
        "    --trained_checkpoint_dir {model_dir} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_config_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgJYPrJRhsce"
      },
      "source": [
        "!zip -r /content/saved_model.zip /content/inference_graph/saved_model/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2d5UKCXhwpH"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/saved_model.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa428iRvkH8T"
      },
      "source": [
        "category_index = label_map_util.create_category_index_from_labelmap(labelmap_path, use_display_name=True)\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.saved_model.load(f'{output_directory}/saved_model')\n",
        "import pandas as pd\n",
        "test = pd.read_csv('/content/images/test_labels.csv')\n",
        "#Getting 3 random images to test\n",
        "images = list(test.sample(n=3)['filename'])\n",
        "print(images)\n",
        "\n",
        "for image_name in images:\n",
        "  \n",
        "  image_np = load_image_into_numpy_array('/content/images/test/' + image_name)\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "  display(Image.fromarray(image_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSrDP6N5AKSK"
      },
      "source": [
        "!rm -rf /content/images/test\n",
        "!rm -rf /content/images/train\n",
        "!rm /content/images/*.csv\n",
        "!rm /content/images/*.pbtxt\n",
        "!rm /content/images/*.record\n",
        "!mkdir /content/images/test\n",
        "!mkdir /content/images/train\n",
        "!mkdir /content/inference_graph\n",
        "!!cp /content/ssd_mobilenet_v2_320x320_coco17_tpu-8.config /content/mobilenet_v2.config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U41vCpf5MqnY"
      },
      "source": [
        "%mkdir /content/training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VX_GqCskgL0K"
      },
      "source": [
        "%mkdir /content/inference_graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMGv_MOBlbLn"
      },
      "source": [
        "!pip install tensorflowjs[wizard]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsDODjbGmLeR"
      },
      "source": [
        "!pip install tensorflowjs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TMgjEKXmQ6Y"
      },
      "source": [
        "!tensorflowjs_converter \\\n",
        "    --input_format=tf_saved_model \\\n",
        "    --output_format=tfjs_graph_model \\\n",
        "    --signature_name=serving_default \\\n",
        "    --saved_model_tags=serve \\\n",
        "    /content/inference_graph/saved_model \\\n",
        "    /content/web_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpatloParqud"
      },
      "source": [
        "imported = tf.saved_model.load('/content/inference_graph/saved_model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yQqirMjt5fr"
      },
      "source": [
        "print(imported)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd2nIUnNlrBM"
      },
      "source": [
        "!zip -r /content/web_model.zip /content/web_model/\n",
        "from google.colab import files\n",
        "files.download(\"/content/web_model.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKqT6FvYofNn"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BxLM0YJ4fE6"
      },
      "source": [
        "\n",
        "\n",
        "*   Return tensorflow version in route `/hello`\n",
        "*   Flask is a web application framework. Please try to find a way to write a POST method and write an API to add two number. You can test the API using `axois` in the html file. Please test the API in index.html file. \n",
        "\n"
      ]
    }
  ]
}